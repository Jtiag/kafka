Kafka初识

#表示root用户命令
$表示普通用户命令
##表示注释

===========================================
产生的背景：
  假设现在有两个模块，分别是A和B，其中A模块产生数据(消息), B模块根据A模块产生的数据进行操作
  问题：
    A模块产生数据的速度远远大于B模块的数据消费速度
    =====> A模块产生的数据有阻塞/有剩余
    =====> 对数据的处理性能有一定的影响
  解决方案：
    A模块发送数据到队列 -----> 中间队列  ------> B模块从队列中获取数据
    中间队列: 形成了一种信息系统的中间体的结构, 消息队列/消息系统
    =====> Kafka是消息队列的一种实现

=======================================
Kafka
  Apache基金会的顶级项目，由LinkedIn公司开源
  定义：
  	Kafka是一个分布式的流处理平台
  	Kafka是一个分布式的、分区的、容错的日志收集系统
  特性：
    1. 允许分布订阅消息，类似于一个消息系统
    2. 高容错的存储数据
    3. 高可扩展性
    4. 高容错性
    5. 快速
    6. 分布式

Kafka版本
  选择：Kafka0.8.2版本
  帮助文档：http://kafka.apache.org/082/documentation.html
  相关概念：
    Message：
    Broker：一般情况一台服务器一个broker，但是可以部署多个,反应到具体的进程就是Kafka进程
    Topic：是Kafka中一组消息的一个整体概念，Produce将消息写入到对应的Topic，Consumer从对应的Topic读取消息
    Partition：一个Topic包含多个分区，Produce发送到Topic的数据根据key的不同发送到不同的partition中；一个分区中的数据有两个特性：
      有序：按照进入kafka的时间排序
      数据不可变：进入到kafka集群的数据不可进行变动
    Producer：数据生产者
    Consumer：数据消费者
    ConsumerGroup：多个Consumer共同进行数据消费，然后多个Consumer之间形成负载均衡的一个特性；一个ConsumerGroup中如果Consumer的数量和消费的Topic的Partition的数量一样多，那么每个Consumer消费一个Partition的数据，如果数据超过Partition的数量，那么有部分consumer处于不消费数据的状态

Kafka0.10.x版本新特性
   Streams API ：支持直接在Kafka上进行流式数据的处理
   Connector API ：直接将Kafka数据持久化到数据存储系统或者从数据存储系统将数据读入Kafka中

Kafka支持语言：
   https://cwiki.apache.org/confluence/display/KAFKA/Clients

Kafka安装
  安装方式：
    单机：一台服务器而且只有一个broker
    伪分布式：一台服务器，但是有多个broker
    完全分布式：多台服务器
  搭建步骤：http://kafka.apache.org/082/documentation.html#quickstart
  前提条件：
    1. 搭建jdk环境 => jdk1.7.x
    2. 搭建scala环境 => scala2.10.x
    3. 搭建zookeeper
  安装步骤：(伪分布式)
    注意：Kafka配置相关参数参考：http://kafka.apache.org/082/documentation.html#brokerconfigs
    1. 下载Kafka安装压缩包
      Apache：(根据scala的版本下载对应的kafka压缩包)
        http://archive.apache.org/dist/kafka
        http://archive.apache.org/dist/kafka/0.8.2.1/
        http://archive.apache.org/dist/kafka/0.8.2.1/kafka_2.10-0.8.2.1.tgz(课程选择的版本url)
      Cloudera:(基本不用)
        http://archive.cloudera.com/kafka/kafka/
    2. 将下载的tgz压缩包或者文件夹"04_软件工具"里面的kafka tgz压缩包上传到LINUX的/opt/softwares/
    3. 解压kafka_2.10-0.8.2.1.tgz到/opt/cdh-5.3.6， 并创建一个软连接
      $ tar -zxvf /opt/softwares/kafka_2.10-0.8.2.1.tgz -C /opt/cdh-5.3.6
      $ ln -s /opt/cdh-5.3.6/kafka_2.10-0.8.2.1/ /opt/cdh-5.3.6/kafka
      $ cd /opt/cdh-5.3.6/kafka
    4. 更改config文件夹中的Kafka服务配置项：server.properties文件中的信息
      $ vim config/server.properties
broker.id=0 ## 给定broker的id的值，在一个kafka集群中该参数必须唯一
port=9092 ## 监听的端口号，默认9092，需要保证改端口没有被使用
host.name=hadoop-senior02.ibeifeng.com ## 监听的主机名，默认是localhost，需要更改为hostname
log.dirs=/opt/cdh-5.3.6/kafka/data/0 ## 指定kafka存储磁盘的路径，可以使用","分割，给定多个磁盘路径；如果服务器挂载多个磁盘，可以将kafka的数据分布存储到不同的磁盘中(每个磁盘写一个路径)，对于Kafka的数据的读写效率有一定的提升（场景：高并发、大数据量的情况）
zookeeper.connect=hadoop-senior02.ibeifeng.com:2181/kafka ## 配置kafka连接zk的相关信息，连接url以及kafka数据存储的zk根目录；这里的配置含义是：连接hadoop-senior02机器2181端口的zookeeper作为kafka的元数据管理zk，zk中使用/kafka作为kafka元数据存储的根目录，默认kafka在zk中的根目录是zk的顶级目录("/")
	5. 复制server.properties, 产生server1.properties、server2.properties、server3.properties三个配置文件，并修改参数：broker.id&port&log.dirs
	  $ cd conf
	  $ cp server.properties server1.properties
	  $ cp server.properties server2.properties
	  $ cp server.properties server3.properties
	6. 启动Kafka服务
	  $ cd /opt/cdh-5.3.6/zookeeper
	  $ bin/zkServer.sh start ## 启动zk
	  $ cd /opt/cdh-5.3.6/kafka ## 开始启动, 开启四个不同的shell客户端
	  $ bin/kafka-server-start.sh config/server.properties 
	  $ bin/kafka-server-start.sh config/server1.properties
	  $ bin/kafka-server-start.sh config/server2.properties
	  $ bin/kafka-server-start.sh config/server3.properties
	7. Kafka服务关闭
	  $ cd /opt/cdh-5.3.6/kafka
	  $ bin/kafka-server-stop.sh ## 会将当前机器上的所有broker全部关闭
	  ## 如果想关闭某个broker，可以通过shell命令查看到pid后，通过kill命令关闭进程

[beifeng@hadoop-senior02 kafka]$ bin/kafka-server-start.sh 
USAGE: bin/kafka-server-start.sh [-daemon] server.properties
===> 启动kafka需要给定kafka是server的配置信息
===> 参数-daemon的含义是将kafka是否启动到后台，如果给定，kafka进程后台启动
===>
  方式一: bin/kafka-server-start.sh config/server.properties
  方式二：bin/kafka-server-start.sh -daemon config/server.properties

Kafka的日志，默认存储在${KAFKA_HOME}/logs文件夹中；如果需要修改文件存储路径，修改log4j.properties
  kafka.logs.dir=logs ## 给定一个具体的存储路径即可

Kafka在zk中的元数据的描述
  consumers：连接kafka的消费者信息，包括消费者&读取数据的偏移量
  config: topic的配置信息
  controller：topic分区各个副本直接leader选举时候用到的一个目录
  admin：kafka管理相关信息
  brokers：kafka集群中broker的相关描述信息
  controller_epoch：和controller类似

============================
Kafka基本命令讲解
  http://kafka.apache.org/082/documentation.html#quickstart
  1. 启动服务
$ bin/kafka-server-start.sh -daemon config/server.properties 
$ bin/kafka-server-start.sh -daemon config/server1.properties
$ bin/kafka-server-start.sh -daemon config/server2.properties
$ bin/kafka-server-start.sh -daemon config/server3.properties
  2. 查看topic操作脚本的相关帮助信息
创建Topic的时候给定的configuration可以参考http://kafka.apache.org/082/documentation.html#configuration中的Topic-level configuration栏目的内容

$ bin/kafka-topics.sh --help
Create, delete, describe, or change a topic.
创建、删除、展示详细信息或者改变topic的脚本
  
  3. 创建Topic
$ bin/kafka-topics.sh --create --topic beifeng0 --zookeeper hadoop-senior02.ibeifeng.com:2181/kafka --config max.message.bytes=12800000 --config flush.messages=1 --partitions 5 --replication-factor 3
$ bin/kafka-topics.sh --create --topic beifeng1 --zookeeper hadoop-senior02.ibeifeng.com:2181/kafka --replication-factor 1 --partitions 1 ## 最少参数创建topic
$ bin/kafka-topics.sh --create --topic beifeng2 --zookeeper hadoop-senior02.ibeifeng.com:2181/kafka --replication-factor 1 --partitions 2
$ bin/kafka-topics.sh --create --topic beifeng3 --zookeeper hadoop-senior02.ibeifeng.com:2181/kafka --replication-factor 1 --partitions 1 --config segment.bytes=10240


  4. 查看当前kafka集群中Topic的情况
$ bin/kafka-topics.sh --zookeeper hadoop-senior02.ibeifeng.com:2181/kafka --list

  5. 查看Topic的详细信息
$ bin/kafka-topics.sh --zookeeper hadoop-senior02.ibeifeng.com:2181/kafka --describe 

$ bin/kafka-topics.sh --zookeeper hadoop-senior02.ibeifeng.com:2181/kafka --describe --topic beifeng0
  
  6. 修改Topic信息
$ bin/kafka-topics.sh --zookeeper hadoop-senior02.ibeifeng.com:2181/kafka --alter --topic beifeng1 --config max.message.bytes=128000
$ bin/kafka-topics.sh --zookeeper hadoop-senior02.ibeifeng.com:2181/kafka --alter --topic beifeng1 --delete-config max.message.bytes
$ bin/kafka-topics.sh --zookeeper hadoop-senior02.ibeifeng.com:2181/kafka --alter --topic beifeng1 --partitions 10 
$ bin/kafka-topics.sh --zookeeper hadoop-senior02.ibeifeng.com:2181/kafka --alter --topic beifeng1 --partitions 3 ## 分区数量只允许增加，不允许减少
$ bin/kafka-topics.sh --zookeeper hadoop-senior02.ibeifeng.com:2181/kafka --alter --topic beifeng2 --replica-assignment 0:1:2,1:2:3 ## 命令行提示是可以运行，但是结果没有改变，原因可能是版本的问题；对于如果说存在分布副本数不够的情况，可以通过改命令增加副本数

  7. 删除Topic
$bin/kafka-topics.sh --delete --topic beifeng2  --zookeeper hadoop-senior02.ibeifeng.com:2181/kafka
## Note: This will have no impact if delete.topic.enable is not set to true.## 默认情况下，删除是标记删除，没有实际删除这个Topic；如果运行删除Topic，两种方式：
方式一：通过delete命令删除后，手动将本地磁盘以及zk上的相关topic的信息删除即可
方式二：配置server.properties文件，给定参数delete.topic.enable=true，表示允许进行Topic的删除

============================================
测试Kafka集群的消息传递功能
1. 启动服务
2. 启动数据生产者
$ bin/kafka-console-producer.sh --broker-list hadoop-senior02.ibeifeng.com:9092,hadoop-senior02.ibeifeng.com:9093,hadoop-senior02.ibeifeng.com:9094,hadoop-senior02.ibeifeng.com:9095 --topic beifeng0 
3. 启动数据消费者
$ bin/kafka-console-consumer.sh --topic beifeng0 --zookeeper hadoop-senior02.ibeifeng.com:2181/kafka ## 不接收consumer启动前kafka中的数据
$ bin/kafka-console-consumer.sh --topic beifeng0 --zookeeper hadoop-senior02.ibeifeng.com:2181/kafka --from-beginning ## 从头开始接收kafka的数据(全部都接收)
$ bin/kafka-console-consumer.sh --topic beifeng0 --zookeeper hadoop-senior02.ibeifeng.com:2181/kafka --from-beginning --consumer.config consumer.properties --delete-consumer-offsets ## 需要在根目录中创建一个consumer.properties，然后内部给定一个group.id， 结合java编写的consumerapi可以进行测试操作

==========================================
Kafka生产者讲解
  数据发送方式：Push的方式
  相关参数配置参考：http://kafka.apache.org/082/documentation.html#producerconfigs

Kafka消费者讲解
  两种方式：队列和发布订阅
    ==> 通过consumer的group.id参数来决定的，当多个consumer的group.id是一样的情况下，是队列模式，这个时候相同group.id的consumer组成一个consumer group组；如果不一样，此时是发布订阅模式
  前提：Kafka中的数据是每个分区中数据是有序的(按照进入kafka集群的时间)
  ====> Consumer在消费Topic的数据的时候，每个分区的数据是按照顺序进行消费的
  获取数据的方式：通过Poll的方式从Kafka获取数据

Push和Poll方式的区别：
  Push是指将数据发送给第三方，不考虑第三方的框架的数据处理能力
  Poll是指根据自己的处理能力，从数据产生发获取数据

=======================================
Kafka高可用性
  1. Kafka Partition Replication
    功能：增加Topic分区的可用性
    每个Partition分为leader和follower两部分(前提是replication factor大于1的)
eg: Topic: beifeng0 Partition: 0    Leader: 3       Replicas: 3,0,1 Isr: 3,0,1 
 ====> leader是brokerid为3的服务，Replicas：复制节点3,0,1; Isr: 3,0,1 指定当前partition活跃的partition(leader + 活跃follower)
     建议：一般情况下，每个分区的replication factor为3-5比较合适；通过shell脚本监控分区的变化，当有效分区数量过少的时候，通知开发人员手动参与进去修改分区节点数
  2. Kafka Leader Election
    方式：通过ISR来实现Leader的选择
    当leader宕机的时候，会从isr列表中选择一个节点来进行恢复；原则：当一个partition宕机或者落后数据太多，leader会将该partition的broker标识符从isr列表中删除；也就是只有isr中的节点才有可能成为主节点
    当leader宕机的时候，其它有效的partition会在zk中创建一个文件夹目录(只会有一个follower节点创建成功)，创建成功的follower节点成为新的leader节点
    当原来的leader节点重新恢复后，会成为一个新的follower节点，添加到ISR列表中(会同步数据)
    内部是基于zk的watch机制来实现通知以及文件夹的创建的 
  3. Message Delivery Semantics
    消息传输协议/类型 ===> 是对于消息系统中消息传递可靠性保障的一个定义
    三种类型的定义：
      -1. At most once: 最多发送一次，允许数据丢失，但是不允许的数据重复的情况下使用该方式
      -2. At least once: 最少发送一次，允许数据重复，但是不允许数据丢失的情况下使用该方式
      -3. Exactly once: 仅发送一次数据，但是不允许存储数据丢失和重复的情况，有可能存储数据发送失败的情况
    Kafka中实现数据传输的可靠性方式：
      数据存储过程中的可靠性保证：
        通过Kafka Partition Replication以及Kafka Leader Election来保证的
      Producer端：
        功能：生产者将数据发送给kafka
        可靠性实现方式：通过参数request.required.acks来决定的，acks是生产者等待kafka集群的接收确认返回值，主要有三个参数：
          -1. 0: 表示Producer不等待Kafka的返回结果，有可能存在数据丢失
          -2. 1: 表示Producer等待Kafka的一个分区的保存成功的返回结果，有可能数据在分区的备份分区copy数据的时候宕机，导致数据丢失
          -3. -1：表示Producer等待Kafka Topic的所有分区返回保存数据成功的标志
      Consumer端：
        功能：消费Kafka中对应Topic中的数据
        可靠性保证：
          每个Parition中的数据是有序的，每条数据在每个Partiiton中都存在一个offset偏移量(数据是按照offset递增的顺序排列的)， Kafka中的数据是否被某个Consumer消费，就根据该Consumer的Offset的值决定数据是否会被消费；offset表示了consumer消费偏移量小于offset的数据，大于等于offset的数据是没有被消费的

===========================================
消息系统是否可以使用
  1. 核心要求：消息不丢失 ===> 可靠性
    1.1 发送消息
      => 问题：重复发送、消息丢失
    1.2 消息处理
      => 问题：消息重复处理、消息漏处理
  2. 消息传输速度是否足够快 ==> 快速
  3. 消息系统能够标识消息是否被处理 ==> 希望不重复处理消息，kafka中对应同一个consumer group通过offset偏移量实现了消息不重复处理的机制
  4. 可以非常简单进行扩容 ===> 可扩展性

============================================
Why Kafka is Fast?? ==> 为什么kafka的性能比较高
 1. 消息集(message set)
   生产者在将消息发送给Kafka的时候，可以将多条消息以前发送，减低IO的次数
 2. 二进制传输
   Kafka的消息传递过程中，会将数据转换为字节数组来进行传递
 3. 顺序读写磁盘
   根据offset递增的顺序读取磁盘，而且每次读取数据是多条数据一起读取的
 4. "零"拷贝
   kafka在传输数据的时候，log文件中的数据直接通过系统内存(内核)直接网络传输，不经过应用(kafka)的内存的数据的交换
 5. 端到端的数据压缩
   kafka支持直接传输压缩的数据(压缩是kafka的Producer Api负责)，kafka接收到producer传输过来的压缩的数据，不会进行解压操作，直接存储；然后当consumer获取数据的时候，直接将压缩好的数据传输过去，consumer接收到压缩数据后再进行解压操作
   Kafka支持默认支持三种压缩方式：lz4(一般不用)、gzip、snappy(最常用)
   配置参数，需要在Producer中进行配置：参数是：compression.codec，参数值默认为none，表示不进行压缩

==============================
Java Kafka Producer实现
  参考：
    http://kafka.apache.org/082/documentation.html#producerapi
    http://kafka.apache.org/081/documentation.html#producerconfigs
    http://kafka.apache.org/081/documentation.html#apidesign
    http://kafka.apache.org/081/documentation.html#producerapi
  参数说明：
    metadata.broker.list：指定kafka监听的主机名和端口号
    request.required.acks：指定producer需要等待多少返回结果
    request.timeout.ms：请求过期时间
    producer.type：异步还是同步，默认sync
    serializer.class： 指定消息的编码器，将消息转换为byte数组
    key.serializer.class：key的编码器
    partitioner.class：数据分区的分区器，默认hash
    batch.num.messages：当异步提交的时候，批量提交数据的大小
    send.buffer.bytes：发送数据的时候给定客户端的缓冲区大小
  参数在代码中主要是三个类：
    kafka.producer.ProducerConfig
    kafka.producer.SyncProducerConfig
    kafka.producer.SyncProducer
  编写一个Java的Kafka数据生产者出来，要求：
    1. 支持多线程写出数据
    2. 给定规则进行数据分区
    3. 采用异步的方式进行数据传递
  注意：
    java开发使用kafka.javaapi.producer.Producer
    metadata.broker.list、serializer.class必须是给定情况下
    topicName在创建消息的时候就已经给定了 ==> 一个kafka的生成这可以发送数据到多个不同的topic中
    如果自定义数据分区器，必须给定一个构造函数参数是VerifiableProperties的

===========================
Java Kafka Consumer实现
  参考：
    http://kafka.apache.org/081/documentation.html#consumerconfigs
    http://kafka.apache.org/081/documentation.html#highlevelconsumerapi
    http://kafka.apache.org/081/documentation.html#simpleconsumerapi
    http://kafka.apache.org/081/documentation.html#apidesign
  参数说明：
  	group.id：给定当前consumer所属的consumer group的唯一标识符
  	zookeeper.connect：zk的连接url
  	auto.commit.enable：是否自动提交offset，默认为true
  	auto.commit.interval.ms：自动提交offset的间隔时间，默认60s
  	auto.offset.reset：指定consumer初始化的offset是日志的最小偏移量还是最大偏移量，参数可选：smallest和largest
  	zookeeper.session.timeout.ms：zk的会话过期时间，默认6000ms
  	zookeeper.connection.timeout.ms：zk连接过期时间，默认6000ms
  	zookeeper.sync.time.ms：zk同步时间参数，默认2000ms
  参数所在类：
    kafka.consumer.ConsumerConfig
  Kafka中的ConsumerAPI分为两类API
    1. High Level API(高级别的API)
      优点：屏蔽了内部的offset以及数据的操作代码，操作简单
      缺点：可操作性太差，只能通过参数进行修改；由于consumer的offset偏移量是间隔性的提交到ZK中，可能会导致Kafka的数据被重复处理
      类：ConsumerConnector
    2. Lower Level API(低级别的API) => Simple Consumer API
      优点：可操作性强，可以通过代码保证数据一定只消费一次
      缺点：编写的代码太多
      类：SimpleConsumer
  编写一个Java的Kafka数据消费者出来，要求：
    1. 使用High Level Consumer API编写
    2. 使用Simple Consumer API编写
  注意：
    1. kafka.javaapi.consumer.ConsumerConnector
public <K,V> Map<String, List<KafkaStream<K,V>>> 
    createMessageStreams(Map<String, Integer> topicCountMap, Decoder<K> keyDecoder, Decoder<V> valueDecoder);
  -1. topicCountMap: 指定读取数据的topic，从那些topic读取数据；读取数据的线程数量(一般情况和分区数量一样)
  -2. keyDecoder&valueDecoder：key和value进行解码操作的类
  -3. 返回值；返回的map集合中，string表示topic，list指的是每个topic对应的流数据处理器对象；topic对应list的大小一般和topicCountMap中topickey对应的count数量一致(count数量<=分区数量)
public void commitOffsets();
  提交偏移量到zk中；当参数auto.commit.enable为true的时候，会自动提交偏移量

===============================
Kafka和Flume集成
  Flume:
    Agent: Source + Channel + Sink
      Source：收集数据
      Channel: 临时的保存数据
      Sink: 发送数据
  环境：Kafka_2.10_0.8.2.1 & Flume-1.5.0-cdh5.3.6
  参考：(Flume默认版本是最新的，和1.5.0版本的帮助文档有一定的区别，在编写的过程中需要注意)
    http://flume.apache.org/FlumeUserGuide.html
    http://flume.apache.org/FlumeUserGuide.html#kafka-source
    http://flume.apache.org/FlumeUserGuide.html#kafka-sink
    http://flume.apache.org/releases/content/1.5.0/FlumeUserGuide.html(仅做参考，主要根据最新的帮助文档)
  案例一：
    Flume将数据输出到Kafka的对应Topic中
    在${FLUME_HOME}/conf文件夹中创建一个test1.conf文件，给定flume的配置信息
    启动flume的agent：
cd /opt/cdh-5.3.6/flume
bin/flume-ng agent --name a1 --conf ./conf/ --conf-file ./conf/test1.conf -Dflume.root.logger=INFO,console
echo "liuming gerry tom" >> /opt/cdh-5.3.6/access.log
   案例二：
     将Kafka的数据输出到Flume中
     在${FLUME_HOME}/conf文件夹中创建一个test2.conf文件，给定flume的配置信息
cd /opt/cdh-5.3.6/flume
bin/flume-ng agent --name a2 --conf ./conf/ --conf-file ./conf/test2.conf -Dflume.root.logger=INFO,console
    异常：
      java.lang.NoClassDefFoundError: org/apache/zookeeper/Watcher
      ====> 缺少包，Flume的agent运行的时候发现缺少包
      ====> 解决方案：从kafka中将可能依赖的包copy到flume的lib文件夹中
      	cp /opt/cdh-5.3.6/kafka/libs/zookeeper-3.4.6.jar /opt/cdh-5.3.6/flume/lib/

==============================================
Kafka集群管理&监控
  比较常用的三款Kafka监控软件：
    https://github.com/
    1. Kafka Web Console
      监控信息比较全面，但是存在bug：不会释放KAFKA连接，最终可能会导致一个连接不够的问题，生产环境一般不用
      https://github.com/claudemamo/kafka-web-console
    2. Kafka Manager
      雅虎开源，偏向Kafka集群管理，同时实现了集群监控的功能
      https://github.com/yahoo/kafka-manager
    3. KafkaOffsetMonitor
      一个非常简单的监控工具，以jar的形式运行，部署非常简单，只有监控功能
      https://github.com/quantifind/KafkaOffsetMonitor
作业：自己搭建Kafka集群的监控的软件


